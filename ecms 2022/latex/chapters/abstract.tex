\section*{\textbf{ABSTRACT}}

Multilayer neural networks are considered universal approximators applicable to a wide range of problems. There are quite detailed theoretical and applied studies for fully connected networks, while for convolutional networks the results are more scarce. In this paper, we tested the approximating capability of deep neural networks with typical architectures like ConvNet, ResNet, and UNet as applied to classical image processing algorithms. Canny edge detector and grayscale morphological dilation with the disk structuring element were selected as target algorithms. As we have seen, even relatively lightweight neural models are able to approximate a filter with fixed parameters. Since classical algorithms are parameterized, we considered different approaches to the parameterization of the neural networks and found out that even the simplest of them, adding parameters in the input images channels, works well for low parameter count. Also, we measured an inference time of a neural network approximation and a classical implementation of the grayscale dilation with the disk structuring element. Starting from a certain radius, a neural network works faster than an algorithm even on one core of the CPU without fine-tuning the architecture for performance, thus confirming the viability of ConvNets as a differentiable approximation technique for optimization of classical-based methods.