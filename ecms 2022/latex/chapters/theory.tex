\section*{\textbf{RELATED WORK}}

In this section, we will describe the main existing results on the approximation of classical filters, which can be considered as baselines for further research. Since classical algorithms have the important property of natural parametrization, which means they can be customized for specific requirements, we will also consider approaches to the parametrization of neural networks. 

\subsection*{\textbf{Neural network approximation of classical algorithms}}

The first works on the research topic appeared even in the 20th century. For example, \cite{Ridder99Kuwahara} investigated the application of neural networks to nonlinear image processing using Kuwahara filter for image smoothing as a target. The authors experimented with several architectures of a network, including single- and double-layer perceptrons with different sizes of the hidden layers and a specially designed modular neural network. Although the authors managed to train the model to smooth the images, the result was still too far from true Kuwahara filter smoothing. The reason for this could be due to unsuitable model architecture and loss function. 

\cite{Fernandez11} attempted to approximate the Canny and Sobel edge detectors with single-layer perceptron. To take into account the spatial dependence in the images, a pixel and its eight adjacent pixels were fed to the input of the model, which almost corresponds to a convolution with a kernel size of 3x3. The authors didn't manage to achieve a good quality of the approximation, which can probably be explained by the inability of the single-layer model to learn complex dependencies in the data. However, these studies can be taken as a baseline for subsequent experiments.


\cite{Fernandez11, Ridder99Kuwahara}) used neural networks with typical architecture: just a multi-layer perceptron. A totally different approach was proposed by \cite{Zhukovsky2018}. The authors developed a complete reproduction of the Canny, Niblack, and Harris filters using only neural network operations: convolution, pulling, concatenation, etc., so the algorithm's parameters do not need to be selected manually because their optimal values will be determined during training. But this approach has its disadvantage since it requires creating a new architecture for each filter, which is very time-consuming and requires a deep understanding of the algorithm's inner workings. It also does not correlate with the aim of our study. 


\subsection*{\textbf{Parameterization approaches}}

In the previous works, the ability of neural networks to approximate a particular filter with fixed parameters was investigated, and those researches are of more theoretical interest. Even though in real-world applications this approach is acceptable, because a filter with specified parameters is often used, it is necessary to be able to customize the filter parameters when creating and optimizing prototypes of the final system. For example, \cite{CannyOptics} used Canny edge detector to develop a computer vision-based method of pre-alignment of a channel optical waveguide and a lensed fiber, and the values of the algorithm parameters were being tuned while the development stage. Sometimes it is necessary to give a user the ability to specify the system parameters, as in the case of an ANN-based image pre-compensation system, which must adjust to the parameters of a particular person's eye \citep{Precompensation} and thus cannot be retrained for each possible set of parameters. 

%\cite{CannyCloudDetection} used Canny edge detector to build a system for automatic recognition of cumuliform cloud, and the values of the algorithm parameters were selected experimentally while the development stage. Sometimes it is necessary to give a user the ability to specify the system parameters, as in the case of an ANN-based image pre-compensation system, which must adjust to the parameters of a particular person's eye \citep{Precompensation} and thus cannot be retraining for each possible set of parameters.  

  
%However, in practical application it is impossible to train a separate model for each separate set of parameters, so it is necessary to build a model that could approximate the filter with any values of parameters from its domain. 

Model parameterization can be achieved in several ways:


  $(1)$  by adding parameters values as metadata to the input image channels. This approach was used by \cite{MultipleInputNN_SoilClay2020}. In order to enrich the data and improve the quality of the model predicting the clay content of the soil, not only remotely sensed values, which constitute the spectral input channel, but also geographic coordinates, which constitute the auxiliary input channels, are fed as an input. This is the simplest way to provide dependence of the model on auxiliary parameters, which does not entail a significant increase in the size of the model, but in some cases, it might be not enough to learn more complex dependencies between parameters and expected output.
  
  $(2)$  by parameterizing the convolution kernel. Traditional convolutional kernels are shared for all examples in a dataset, which can significantly decrease the quality of the model in case the data are characterized by a large variability. Conditional Convolutions (CondConv), proposed by \cite{CondConv2020}, and Dynamic Convolutions (DynamicConv), proposed by \cite{DynamicConv2020}), share a common idea: instead of using a single convolution kernel per layer, which is the same for any input, these types of convolutions aggregates multiple convolution kernels, which are input dependent. DynamicConv aggregates several kernels based upon their attentions while CondConv parameterizes the convolutional kernels as a linear combination with example-dependent scalar weights computed using a routing function with learned parameters. Another example of this approach is the Adaptive convolutional neural network (ACNN), presented in \cite{AdaptiveConvolutions2017}. In ACNN the filter weights are generated with a learned sub-network, which is just a multilayer perceptron and whose input is the side information or metadata. Replacing traditional convolution layers by convolutions with adaptive weights can disentangle the variations related to the side information and extract discriminative features related to the current context, but it can significantly increase the size of the model and slow down its inference time.  

  $(3)$  by adding extra input layers for metadata to the neural network. \cite{MultipleInputNN_Water2020} and \cite{EfficientNetMetaData2020} used a multiple-input neural network, which consists of two branches: the first one is a single- or multilayer perceptron that process a vector of metadata and the second one is a convolutional neural network that process an input image. The outputs of these sub-networks are then concatenated and processed with the common part of the network. \cite{Plankton2019} developed this idea and proposed several schemes for metadata incorporation. While in the previous three works metadata was added to a convolutional neural network, \cite{GCN2021covid19} demonstrated how to add metadata to a graph neural network.
  


% \begin{itemize}
%   \item[--] by adding parameters values as metadata to the input image channels. This approach was used in \cite{MultipleInputNN_SoilClay2020}. In order to enrich the data and improve the quality of the model predicting the clay content of the soil, not only remotely sensed values, which constitute the spectral input channel, but also geographic coordinates, which constitute the auxiliary input channels, are fed as an input. This is the simplest way to provide dependence of the model on auxiliary parameters, which does not entail a significant increase in the size of the model, but in some cases, it might be not enough to learn more complex dependencies between parameters and expected output.
  
%   \item[--] by parameterizing the convolution kernel. Traditional convolutional kernels are shared for all examples in a dataset, which can significantly decrease the quality of the model in case the data are characterized by a large variability. Conditional Convolutions (CondConv), proposed by \cite{CondConv2020}, and Dynamic Convolutions (DynamicConv), proposed by \cite{DynamicConv2020}), share a common idea: instead of using a single convolution kernel per layer, which is the same for any input, these types of convolutions aggregates multiple convolution kernels, which are input dependent. DynamicConv aggregates several kernels based upon their attentions while CondConv parameterizes the convolutional kernels as a linear combination with example-dependent scalar weights computed using a routing function with learned parameters. Another example of this approach is the Adaptive convolutional neural network (ACNN) presented in \cite{AdaptiveConvolutions2017}. In ACNN the filter weights are generated with a learned sub-network, which is just a multilayer perceptron and whose input is the side information or metadata. Replacing traditional convolution layers by convolutions with adaptive weights can disentangle the variations related to the side information and extract discriminative features related to the current context, but it can significantly increase the size of the model and slow down its inference time.  

%   \item[--] by adding extra input layers for metadata to the neural network. \cite{MultipleInputNN_Water2020} and \cite{EfficientNetMetaData2020} used a multiple-input neural network, which consists of two branches: the first one is a single- or multilayer perceptron that process a vector of metadata and the second one is a convolutional neural network that process an input image. The outputs of these sub-networks are then concatenated and processed with the common part of the network. \cite{Plankton2019} developed this idea and proposed several schemes for metadata incorporation. While in the previous three works metadata was added to a convolutional neural network, \cite{GCN2021covid19} demonstrated how to add metadata to a graph neural network.
  
% \end{itemize}