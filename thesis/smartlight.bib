@book{MinskyPapert69,
  added-at = {2013-12-05T10:32:26.000+0100},
  address = {Cambridge, MA},
  author = {Minsky, M. and Papert, S.},
  biburl = {https://www.bibsonomy.org/bibtex/24587aec0472c41d00c38bf3e888304ba/prlz77},
  interhash = {9ad5b73f68093070d73e54312145eca2},
  intrahash = {4587aec0472c41d00c38bf3e888304ba},
  keywords = {critic minsky papert perceptron},
  publisher = {MIT Press},
  timestamp = {2013-12-05T10:32:26.000+0100},
  title = {Perceptrons},
  year = 1969
}

@book{LeCun1987,
title = "PhD thesis: Modeles connexionnistes de l'apprentissage (connectionist learning models)",
author = "Yann Lecun",
year = "1987",
month = jun,
language = "English (US)",
publisher = "Universite P. et M. Curie (Paris 6)",
}

@inproceedings{LapedesFarber1987,
author = {Lapedes, Alan and Farber, Robert},
year = {1987},
month = {01},
pages = {442-456},
title = {How Neural Nets Work},
isbn = {978-9971-5-0529-5},
doi = {10.1142/9789814434102_0012}
}

@article{IrieMiyake88,
  title={Capabilities of three-layered perceptrons},
  author={Bito Irie and Satoshi Miyake},
  journal={IEEE 1988 International Conference on Neural Networks},
  year={1988},
  pages={641-648 vol.1}
}

@article{Hornik89,
title = {Multilayer feedforward networks are universal approximators},
journal = {Neural Networks},
volume = {2},
number = {5},
pages = {359-366},
year = {1989},
issn = {0893-6080},
doi = {https://doi.org/10.1016/0893-6080(89)90020-8},
url = {https://www.sciencedirect.com/science/article/pii/0893608089900208},
author = {Kurt Hornik and Maxwell Stinchcombe and Halbert White},
keywords = {Feedforward networks, Universal approximation, Mapping networks, Network representation capability, Stone-Weierstrass Theorem, Squashing functions, Sigma-Pi networks, Back-propagation networks},
abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.}
}


@article{Ridder99Kuwahara,
author = {De Ridder, D. and Duin, R. P. W. and Verbeek, P. W. and Van Vliet, L. J.},
title = {The Applicability of Neural Networks to Non-Linear Image Processing},
year = {1999},
issue_date = {June 1999},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {2},
number = {2},
issn = {1433-7541},
url = {https://doi.org/10.1007/s100440050022},
doi = {10.1007/s100440050022},
abstract = {In this paper, the applicability of neural networks to non-linear image processing
problems is studied. As an example, the Kuwahara filtering for edge-preserving smoothing
was chosen. This filter is interesting due to its non-linear nature and natural modularity.
A number of modular networks were constructed and trained, incorporating prior knowledge
in various degrees and their performance was compared to standard feed-forward neural
networks (MLPs). Based on results obtained in these experiments, it is shown that
several key factors influence neural network behaviour in this kind of task. First,
it is demonstrated that the mean squared error criterion used in neural network training
is not representative for the problem. To be able to discern performance differences
better, a new error measure for edge-preserving smoothing operations is proposed.
Secondly, using this measure, it is shown that modular networks perform better than
standard feed-forward networks. The latter type often ends up in linear approximations
to the filter. Finally, inspection of the modular networks shows that, although analysis
is difficult due to their non-linearity, one can draw some conclusions regarding the
effect of design and training choices. The main conclusion is that neural networks
can be applied to non-linear image processing problems, provided that careful attention
is paid to network architecture, training set sampling and parameter choice. Only
if prior knowledge is used in constructing the networks and sampling the datasets
can one expect to obtain a well performing neural network filter.},
journal = {Pattern Anal. Appl.},
month = jun,
pages = {111–128},
numpages = {18},
keywords = {Key words: Edge preserving smoothing; Image processing; Neural network architectures; Non-linear filtering; Quantitative performance measures}
}


@article{Fernandez11,
author = {Fernández, Andrea and Delgado-Mata, Carlos and Velázquez, Ramiro},
year = {2011},
month = {11},
pages = {},
title = {Training a Single-Layer Perceptron for an Approximate Edge Detection on a Digital Image},
journal = {Proceedings - 2011 Conference on Technologies and Applications of Artificial Intelligence, TAAI 2011},
doi = {10.1109/TAAI.2011.40}
}


@article{Zhou20,
title = {Universality of deep convolutional neural networks},
journal = {Applied and Computational Harmonic Analysis},
volume = {48},
number = {2},
pages = {787-794},
year = {2020},
issn = {1063-5203},
doi = {https://doi.org/10.1016/j.acha.2019.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S1063520318302045},
author = {Ding-Xuan Zhou},
keywords = {Deep learning, Convolutional neural network, Universality, Approximation theory},
abstract = {Deep learning has been widely applied and brought breakthroughs in speech recognition, computer vision, and many other domains. Deep neural network architectures and computational issues have been well studied in machine learning. But there lacks a theoretical foundation for understanding the approximation or generalization ability of deep learning methods generated by the network architectures such as deep convolutional neural networks. Here we show that a deep convolutional neural network (CNN) is universal, meaning that it can be used to approximate any continuous function to an arbitrary accuracy when the depth of the neural network is large enough. This answers an open question in learning theory. Our quantitative estimate, given tightly in terms of the number of free parameters to be computed, verifies the efficiency of deep CNNs in dealing with large dimensional data. Our study also demonstrates the role of convolutions in deep CNNs.}
}

@article{Petersen2018EquivalenceOA,
  title={Equivalence of approximation by convolutional neural networks and fully-connected networks},
  author={Philipp Christian Petersen and Felix Voigtl{\"a}nder},
  journal={ArXiv},
  year={2018},
  volume={abs/1809.00973}
}

@article{Jampani2016LearningSH,
  title={Learning Sparse High Dimensional Filters: Image Filtering, Dense CRFs and Bilateral Neural Networks},
  author={V. Jampani and Martin Kiefel and Peter V. Gehler},
  journal={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2016},
  pages={4452-4461}
}

@InProceedings{Yi2016LearnedIFT,
author="Yi, Kwang Moo
and Trulls, Eduard
and Lepetit, Vincent
and Fua, Pascal",
editor="Leibe, Bastian
and Matas, Jiri
and Sebe, Nicu
and Welling, Max",
title="LIFT: Learned Invariant Feature Transform",
booktitle="Computer Vision -- ECCV 2016",
year="2016",
publisher="Springer International Publishing",
address="Cham",
pages="467--483",
abstract="We introduce a novel Deep Network architecture that implements the full feature point handling pipeline, that is, detection, orientation estimation, and feature description. While previous works have successfully tackled each one of these problems individually, we show how to learn to do all three in a unified manner while preserving end-to-end differentiability. We then demonstrate that our Deep pipeline outperforms state-of-the-art methods on a number of benchmark datasets, without the need of retraining.",
isbn="978-3-319-46466-4"
}

@INPROCEEDINGS{Febbo2018KCNN,
  author={Di Febbo, Paolo and Dal Mutto, Carlo and Tieu, Kinh and Mattoccia, Stefano},
  booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)}, 
  title={KCNN: Extremely-Efficient Hardware Keypoint Detection with a Compact Convolutional Neural Network}, 
  year={2018},
  volume={},
  number={},
  pages={795-7958},
  doi={10.1109/CVPRW.2018.00111}}
  
  
@article{Zhukovsky2018,
  title={Реализация классических алгоритмов анализа изображений через полносверточные нейронные сети},
  author={Жуковский, А.Е., Лимонова, Е.Е., Николаев, Д.П.},
  journal={Труды ИСА РАН. Спецвыпуск,},
  year={2018},
  pages={108-116 №.1}
}

@article{Wibisono2020TraditionalMI,
  title={Traditional Method Inspired Deep Neural Network For Edge Detection},
  author={Jan Kristanto Wibisono and Hsueh-Ming Hang},
  journal={2020 IEEE International Conference on Image Processing (ICIP)},
  year={2020},
  pages={678-682}
}


@article{Soria2020DenseEI,
  title={Dense Extreme Inception Network: Towards a Robust CNN Model for Edge Detection},
  author={Xavier Soria and Edgar Riba and Angel Domingo Sappa},
  journal={2020 IEEE Winter Conference on Applications of Computer Vision (WACV)},
  year={2020},
  pages={1912-1921}
}


@inproceedings{Niu2020SingleIS,
  title={Single Image Super-Resolution via a Holistic Attention Network},
  author={Ben Niu and Weilei Wen and Wenqi Ren and Xiangde Zhang and Lianping Yang and Shuzhen Wang and Kaihao Zhang and Xiaochun Cao and Haifeng Shen},
  booktitle={ECCV},
  year={2020}
}


@article{Sheng2021CTNet,
author = {Sheng, He and Schomaker, Lambert},
year = {2021},
month = {05},
pages = {},
title = {CT-Net: Cascade T-Shape Deep Fusion Networks for Document Binarization},
volume = {118},
journal = {Pattern Recognition},
doi = {10.1016/j.patcog.2021.108010}
}